{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pettingzoo.utils import aec_to_parallel, parallel_to_aec\n",
    "from pettingzoo.utils.wrappers import AssertOutOfBoundsWrapper, OrderEnforcingWrapper\n",
    "\n",
    "from env.aquarium import raw_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def env2(\n",
    "    render_mode: str = \"human\",\n",
    "    observable_walls: int = 2,\n",
    "    width: int = 800,\n",
    "    height: int = 800,\n",
    "    caption: str = \"Aquarium\",\n",
    "    fps: int = 60,\n",
    "    max_time_steps: int = 3000,\n",
    "    action_count: int = 16,\n",
    "    predator_count: int = 1,\n",
    "    prey_count: int = 16,\n",
    "    predator_observe_count: int = 1,\n",
    "    prey_observe_count: int = 3,\n",
    "    draw_force_vectors: bool = False,\n",
    "    draw_action_vectors: bool = False,\n",
    "    draw_view_cones: bool = False,\n",
    "    draw_hit_boxes: bool = False,\n",
    "    draw_death_circles: bool = False,\n",
    "    fov_enabled: bool = True,\n",
    "    keep_prey_count_constant: bool = True,\n",
    "    prey_radius: int = 20,\n",
    "    prey_max_acceleration: float = 1,\n",
    "    prey_max_velocity: float = 4,\n",
    "    prey_view_distance: int = 100,\n",
    "    prey_replication_age: int = 200,\n",
    "    prey_max_steer_force: float = 0.6,\n",
    "    prey_fov: int = 120,\n",
    "    prey_reward: int = 1,\n",
    "    prey_punishment: int = 1000,\n",
    "    max_prey_count: int = 20,\n",
    "    predator_max_acceleration: float = 0.6,\n",
    "    predator_radius: int = 30,\n",
    "    predator_max_velocity: float = 5,\n",
    "    predator_view_distance: int = 200,\n",
    "    predator_max_steer_force: float = 0.6,\n",
    "    predator_max_age: int = 3000,\n",
    "    predator_fov: int = 150,\n",
    "    predator_reward: int = 10,\n",
    "    catch_radius: int = 100,\n",
    "    procreate: bool = False,\n",
    "):\n",
    "    \"\"\"Returns the AEC environment\"\"\"\n",
    "    env_aec = parallel_to_aec(\n",
    "        raw_env(\n",
    "            render_mode=render_mode,\n",
    "            observable_walls=observable_walls,\n",
    "            width=width,\n",
    "            height=height,\n",
    "            caption=caption,\n",
    "            fps=fps,\n",
    "            max_time_steps=max_time_steps,\n",
    "            action_count=action_count,\n",
    "            predator_count=predator_count,\n",
    "            prey_count=prey_count,\n",
    "            predator_observe_count=predator_observe_count,\n",
    "            prey_observe_count=prey_observe_count,\n",
    "            draw_force_vectors=draw_force_vectors,\n",
    "            draw_action_vectors=draw_action_vectors,\n",
    "            draw_view_cones=draw_view_cones,\n",
    "            draw_hit_boxes=draw_hit_boxes,\n",
    "            draw_death_circles=draw_death_circles,\n",
    "            fov_enabled=fov_enabled,\n",
    "            keep_prey_count_constant=keep_prey_count_constant,\n",
    "            prey_radius=prey_radius,\n",
    "            prey_max_acceleration=prey_max_acceleration,\n",
    "            prey_max_velocity=prey_max_velocity,\n",
    "            prey_view_distance=prey_view_distance,\n",
    "            prey_replication_age=prey_replication_age,\n",
    "            prey_max_steer_force=prey_max_steer_force,\n",
    "            prey_fov=prey_fov,\n",
    "            prey_reward=prey_reward,\n",
    "            prey_punishment=prey_punishment,\n",
    "            max_prey_count=max_prey_count,\n",
    "            predator_max_acceleration=predator_max_acceleration,\n",
    "            predator_radius=predator_radius,\n",
    "            predator_max_velocity=predator_max_velocity,\n",
    "            predator_view_distance=predator_view_distance,\n",
    "            predator_max_steer_force=predator_max_steer_force,\n",
    "            predator_max_age=predator_max_age,\n",
    "            predator_fov=predator_fov,\n",
    "            predator_reward=predator_reward,\n",
    "            catch_radius=catch_radius,\n",
    "            procreate=procreate,\n",
    "        )\n",
    "    )\n",
    "    env_aec = AssertOutOfBoundsWrapper(env_aec)\n",
    "    env_aec = OrderEnforcingWrapper(env_aec)\n",
    "\n",
    "    return env_aec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parallel_env(\n",
    "    render_mode: str = \"human\",\n",
    "    observable_walls: int = 2,\n",
    "    width: int = 800,\n",
    "    height: int = 800,\n",
    "    caption: str = \"Aquarium\",\n",
    "    fps: int = 60,\n",
    "    max_time_steps: int = 3000,\n",
    "    action_count: int = 16,\n",
    "    predator_count: int = 1,\n",
    "    prey_count: int = 16,\n",
    "    predator_observe_count: int = 1,\n",
    "    prey_observe_count: int = 3,\n",
    "    draw_force_vectors: bool = False,\n",
    "    draw_action_vectors: bool = False,\n",
    "    draw_view_cones: bool = False,\n",
    "    draw_hit_boxes: bool = False,\n",
    "    draw_death_circles: bool = False,\n",
    "    fov_enabled: bool = True,\n",
    "    keep_prey_count_constant: bool = True,\n",
    "    prey_radius: int = 20,\n",
    "    prey_max_acceleration: float = 1,\n",
    "    prey_max_velocity: float = 4,\n",
    "    prey_view_distance: int = 100,\n",
    "    prey_replication_age: int = 200,\n",
    "    prey_max_steer_force: float = 0.6,\n",
    "    prey_fov: int = 120,\n",
    "    prey_reward: int = 1,\n",
    "    prey_punishment: int = 1000,\n",
    "    max_prey_count: int = 20,\n",
    "    predator_max_acceleration: float = 0.6,\n",
    "    predator_radius: int = 30,\n",
    "    predator_max_velocity: float = 5,\n",
    "    predator_view_distance: int = 200,\n",
    "    predator_max_steer_force: float = 0.6,\n",
    "    predator_max_age: int = 3000,\n",
    "    predator_fov: int = 150,\n",
    "    predator_reward: int = 10,\n",
    "    catch_radius: int = 100,\n",
    "    procreate: bool = False,\n",
    "):\n",
    "    \"\"\"Returns the parallel environment\"\"\"\n",
    "    return aec_to_parallel(\n",
    "        env(\n",
    "            render_mode=render_mode,\n",
    "            observable_walls=observable_walls,\n",
    "            width=width,\n",
    "            height=height,\n",
    "            caption=caption,\n",
    "            fps=fps,\n",
    "            max_time_steps=max_time_steps,\n",
    "            action_count=action_count,\n",
    "            predator_count=predator_count,\n",
    "            prey_count=prey_count,\n",
    "            predator_observe_count=predator_observe_count,\n",
    "            prey_observe_count=prey_observe_count,\n",
    "            draw_force_vectors=draw_force_vectors,\n",
    "            draw_action_vectors=draw_action_vectors,\n",
    "            draw_view_cones=draw_view_cones,\n",
    "            draw_hit_boxes=draw_hit_boxes,\n",
    "            draw_death_circles=draw_death_circles,\n",
    "            fov_enabled=fov_enabled,\n",
    "            keep_prey_count_constant=keep_prey_count_constant,\n",
    "            prey_radius=prey_radius,\n",
    "            prey_max_acceleration=prey_max_acceleration,\n",
    "            prey_max_velocity=prey_max_velocity,\n",
    "            prey_view_distance=prey_view_distance,\n",
    "            prey_replication_age=prey_replication_age,\n",
    "            prey_max_steer_force=prey_max_steer_force,\n",
    "            prey_fov=prey_fov,\n",
    "            prey_reward=prey_reward,\n",
    "            prey_punishment=prey_punishment,\n",
    "            max_prey_count=max_prey_count,\n",
    "            predator_max_acceleration=predator_max_acceleration,\n",
    "            predator_radius=predator_radius,\n",
    "            predator_max_velocity=predator_max_velocity,\n",
    "            predator_view_distance=predator_view_distance,\n",
    "            predator_max_steer_force=predator_max_steer_force,\n",
    "            predator_max_age=predator_max_age,\n",
    "            predator_fov=predator_fov,\n",
    "            predator_reward=predator_reward,\n",
    "            catch_radius=catch_radius,\n",
    "            procreate=procreate,\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = env2(\n",
    "    # draw_force_vectors=True,\n",
    "    # draw_action_vectors=True,\n",
    "    # draw_view_cones=True,\n",
    "    # draw_hit_boxes=True,\n",
    "    # draw_death_circles=True,\n",
    "    procreate = True\n",
    ")\n",
    "env.reset(seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m         action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space(agent)\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m     11\u001b[0m     env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m---> 12\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/aquarium/lib/python3.10/site-packages/pettingzoo/utils/wrappers/order_enforcing.py:80\u001b[0m, in \u001b[0;36mOrderEnforcingWrapper.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m     EnvLogger\u001b[38;5;241m.\u001b[39merror_render_before_reset()\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_rendered \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/aquarium/lib/python3.10/site-packages/pettingzoo/utils/wrappers/base.py:96\u001b[0m, in \u001b[0;36mBaseWrapper.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/aquarium/lib/python3.10/site-packages/pettingzoo/utils/wrappers/base.py:96\u001b[0m, in \u001b[0;36mBaseWrapper.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/aquarium/lib/python3.10/site-packages/pettingzoo/utils/wrappers/order_enforcing.py:80\u001b[0m, in \u001b[0;36mOrderEnforcingWrapper.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m     EnvLogger\u001b[38;5;241m.\u001b[39merror_render_before_reset()\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_rendered \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/aquarium/lib/python3.10/site-packages/pettingzoo/utils/wrappers/base.py:96\u001b[0m, in \u001b[0;36mBaseWrapper.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/aquarium/lib/python3.10/site-packages/pettingzoo/utils/conversions.py:402\u001b[0m, in \u001b[0;36mparallel_to_aec_wrapper.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/code/task/task/aquarium/marl-aquarium/marl_aquarium/env/aquarium.py:267\u001b[0m, in \u001b[0;36mraw_env.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mview \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mview \u001b[38;5;241m=\u001b[39m View(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcaption, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfps)\n\u001b[0;32m--> 267\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_background\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m screenshot_number \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mget():\n",
      "File \u001b[0;32m~/Downloads/code/task/task/aquarium/marl-aquarium/marl_aquarium/env/view.py:86\u001b[0m, in \u001b[0;36mView.draw_background\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdraw_background\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     85\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Draws the background of the pygame window.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscreen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackground\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclock\u001b[38;5;241m.\u001b[39mtick(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfps)\n\u001b[1;32m     88\u001b[0m     fps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclock\u001b[38;5;241m.\u001b[39mget_fps()\n",
      "\u001b[0;31merror\u001b[0m: display Surface quit"
     ]
    }
   ],
   "source": [
    "for agent in env.agent_iter():\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "    len(env.agents)\n",
    "\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        # this is where you would insert your policy\n",
    "        action = env.action_space(agent).sample()\n",
    "\n",
    "    env.step(action)\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Episode 1\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "class AquariumRunner:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.episode_count = 0\n",
    "        self.step_count = 0\n",
    "        \n",
    "    def run_episode(self):\n",
    "        # 重置环境\n",
    "        self.env.reset()\n",
    "        episode_rewards = {agent: 0 for agent in self.env.possible_agents}\n",
    "        \n",
    "        # 使用agent_iter()来迭代每个agent\n",
    "        for agent in self.env.agent_iter():\n",
    "            # 获取当前agent的状态\n",
    "            observation, reward, termination, truncation, info = self.env.last()\n",
    "            \n",
    "            # 更新奖励\n",
    "            if agent in episode_rewards:\n",
    "                episode_rewards[agent] += reward\n",
    "            \n",
    "            # 决定action\n",
    "            if termination or truncation:\n",
    "                action = None\n",
    "            else:\n",
    "                # 这里可以替换为实际的策略\n",
    "                action = self.env.action_space(agent).sample()\n",
    "            \n",
    "            # 执行action\n",
    "            self.env.step(action)\n",
    "            \n",
    "            try:\n",
    "                self.env.render()\n",
    "            except Exception as e:\n",
    "                print(f\"Render error: {e}\")\n",
    "            \n",
    "            # 更新步数\n",
    "            self.step_count += 1\n",
    "            \n",
    "            # 检查是否所有agent都终止了\n",
    "            if len(self.env.agents) == 0:\n",
    "                break\n",
    "                \n",
    "        self.episode_count += 1\n",
    "        return episode_rewards\n",
    "\n",
    "def run_aquarium(episodes=10, render=True):\n",
    "    try:\n",
    "        # 创建环境\n",
    "        env = env2(\n",
    "            render_mode=\"human\" if render else None,\n",
    "            predator_count=1,\n",
    "            prey_count=16,\n",
    "            max_time_steps=3000\n",
    "        )\n",
    "        \n",
    "        runner = AquariumRunner(env)\n",
    "        \n",
    "        for episode in range(episodes):\n",
    "            print(f\"\\nStarting Episode {episode + 1}\")\n",
    "            \n",
    "            # 运行一个episode\n",
    "            rewards = runner.run_episode()\n",
    "            \n",
    "            # 打印统计信息\n",
    "            print(f\"\\nEpisode {episode + 1} Summary:\")\n",
    "            print(f\"Steps completed: {runner.step_count}\")\n",
    "            print(f\"Active agents: {len(env.agents)}\")\n",
    "            print(\"Rewards:\", {k: round(v, 2) for k, v in rewards.items()})\n",
    "            \n",
    "            # 打印存活统计\n",
    "            predators = len([a for a in env.agents if a.startswith(\"predator\")])\n",
    "            prey = len([a for a in env.agents if a.startswith(\"prey\")])\n",
    "            print(f\"Surviving predators: {predators}\")\n",
    "            print(f\"Surviving prey: {prey}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        \n",
    "    finally:\n",
    "        env.close()\n",
    "\n",
    "# 运行示例\n",
    "if __name__ == \"__main__\":\n",
    "    run_aquarium(episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 229\u001b[0m\n\u001b[1;32m    226\u001b[0m         env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 229\u001b[0m     \u001b[43mtrain_aquarium\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 206\u001b[0m, in \u001b[0;36mtrain_aquarium\u001b[0;34m(episodes)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain_aquarium\u001b[39m(episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m):\n\u001b[1;32m    205\u001b[0m     env \u001b[38;5;241m=\u001b[39m env2(render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, predator_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, prey_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n\u001b[0;32m--> 206\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m \u001b[43mAquariumTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(episodes):\n",
      "Cell \u001b[0;32mIn[13], line 116\u001b[0m, in \u001b[0;36mAquariumTrainer.__init__\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;241m=\u001b[39m env\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# 获取观察空间和动作空间的大小\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredator_obs_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredator_0\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprey_obs_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprey_0\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m  \u001b[38;5;66;03m# 8个方向的动作\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# DQN网络\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# 经验回放缓冲区\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# DQN Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, agent_type, learning_rate=0.001):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.agent_type = agent_type\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # 创建Q网络和目标网络\n",
    "        self.q_network = DQN(state_size, action_size).to(self.device)\n",
    "        self.target_network = DQN(state_size, action_size).to(self.device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        self.memory = ReplayBuffer(10000)\n",
    "        \n",
    "        # 超参数\n",
    "        self.batch_size = 64\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.target_update = 10\n",
    "        self.update_counter = 0\n",
    "        \n",
    "    def select_action(self, state, training=True):\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            q_values = self.q_network(state)\n",
    "            return q_values.argmax().item()\n",
    "    \n",
    "    def train(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # 从经验回放中采样\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        # 转换为tensor\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "        \n",
    "        # 计算当前Q值\n",
    "        current_q = self.q_network(states).gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        # 计算目标Q值\n",
    "        with torch.no_grad():\n",
    "            next_q = self.target_network(next_states).max(1)[0]\n",
    "            target_q = rewards + (1 - dones) * self.gamma * next_q\n",
    "        \n",
    "        # 计算损失并更新\n",
    "        loss = nn.MSELoss()(current_q.squeeze(), target_q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # 更新目标网络\n",
    "        self.update_counter += 1\n",
    "        if self.update_counter % self.target_update == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        # 衰减探索率\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "# 训练环境\n",
    "class AquariumTrainer:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        \n",
    "        # 获取观察空间和动作空间的大小\n",
    "        self.predator_obs_size = len(self.env.reset()[0]['predator_0'])\n",
    "        self.prey_obs_size = len(self.env.reset()[0]['prey_0'])\n",
    "        self.action_size = 8  # 8个方向的动作\n",
    "        \n",
    "        # 创建predator和prey的agents\n",
    "        self.predator_agent = DQNAgent(self.predator_obs_size, self.action_size, \"predator\")\n",
    "        self.prey_agents = {}  # 为每个prey创建一个agent\n",
    "        \n",
    "    def train_episode(self, episode):\n",
    "        # 重置环境\n",
    "        observations = self.env.reset()[0]\n",
    "        episode_rewards = {agent: 0 for agent in self.env.possible_agents}\n",
    "        total_loss = 0\n",
    "        steps = 0\n",
    "        \n",
    "        # 为新的prey创建agents\n",
    "        for agent_id in self.env.agents:\n",
    "            if agent_id.startswith(\"prey\") and agent_id not in self.prey_agents:\n",
    "                self.prey_agents[agent_id] = DQNAgent(self.prey_obs_size, self.action_size, \"prey\")\n",
    "        \n",
    "        # 运行一个episode\n",
    "        for agent in self.env.agent_iter():\n",
    "            observation, reward, termination, truncation, info = self.env.last()\n",
    "            \n",
    "            # 更新奖励\n",
    "            if agent in episode_rewards:\n",
    "                episode_rewards[agent] += reward\n",
    "            \n",
    "            # 选择动作\n",
    "            if termination or truncation:\n",
    "                action = None\n",
    "            else:\n",
    "                if agent.startswith(\"predator\"):\n",
    "                    action = self.predator_agent.select_action(observation)\n",
    "                else:\n",
    "                    action = self.prey_agents[agent].select_action(observation)\n",
    "            \n",
    "            # 执行动作\n",
    "            self.env.step(action)\n",
    "            \n",
    "            # 获取新的状态\n",
    "            next_observation = self.env.observe(agent) if not (termination or truncation) else None\n",
    "            \n",
    "            # 存储经验\n",
    "            if next_observation is not None:\n",
    "                if agent.startswith(\"predator\"):\n",
    "                    self.predator_agent.memory.push(\n",
    "                        observation, action, reward, next_observation, \n",
    "                        termination or truncation\n",
    "                    )\n",
    "                else:\n",
    "                    self.prey_agents[agent].memory.push(\n",
    "                        observation, action, reward, next_observation, \n",
    "                        termination or truncation\n",
    "                    )\n",
    "            \n",
    "            # 训练\n",
    "            if agent.startswith(\"predator\"):\n",
    "                loss = self.predator_agent.train()\n",
    "            else:\n",
    "                loss = self.prey_agents[agent].train()\n",
    "            \n",
    "            if loss is not None:\n",
    "                total_loss += loss\n",
    "            \n",
    "            steps += 1\n",
    "            \n",
    "            # 可选：渲染环境\n",
    "            if episode % 100 == 0:\n",
    "                self.env.render()\n",
    "            \n",
    "            # 检查是否结束\n",
    "            if len(self.env.agents) == 0:\n",
    "                break\n",
    "        \n",
    "        # 计算平均损失\n",
    "        avg_loss = total_loss / steps if steps > 0 else 0\n",
    "        \n",
    "        # 打印训练信息\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"\\nEpisode {episode}\")\n",
    "            print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "            print(f\"Predator Epsilon: {self.predator_agent.epsilon:.4f}\")\n",
    "            print(\"Rewards:\", {k: round(v, 2) for k, v in episode_rewards.items()})\n",
    "            print(f\"Steps: {steps}\")\n",
    "            \n",
    "        return episode_rewards, avg_loss\n",
    "\n",
    "def train_aquarium(episodes=1000):\n",
    "    env = env2(render_mode=\"human\", predator_count=1, prey_count=16)\n",
    "    trainer = AquariumTrainer(env)\n",
    "    \n",
    "    try:\n",
    "        for episode in range(episodes):\n",
    "            rewards, loss = trainer.train_episode(episode)\n",
    "            \n",
    "            # 保存模型（每100个episode）\n",
    "            if episode % 100 == 0:\n",
    "                torch.save(trainer.predator_agent.q_network.state_dict(), \n",
    "                         f'predator_model_ep{episode}.pth')\n",
    "                # 保存一个prey的模型作为示例\n",
    "                if trainer.prey_agents:\n",
    "                    first_prey = next(iter(trainer.prey_agents.values()))\n",
    "                    torch.save(first_prey.q_network.state_dict(), \n",
    "                             f'prey_model_ep{episode}.pth')\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted by user\")\n",
    "    \n",
    "    finally:\n",
    "        env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_aquarium()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aquarium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
